{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eulerian Video Magnification.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC3M4_pfZfV1"
      },
      "source": [
        "# Eulerian Video Magnification\n",
        "\n",
        "<p>Eulerian video magnification reveals temporal variations in videos that are difficult or impossible to see with the naked eye and display them in an indicative manner.\n",
        "</p>\n",
        "<p>We are going to create a system which takes in an input video and outputs a video that is motion magnified. The system first decomposes the input video sequence into different\n",
        "spatial frequency bands, and applies the same temporal filter to all bands. The filtered spatial bands are then amplified by a given factor $\\alpha$,\n",
        "added back to the original signal, and collapsed to generate the output video.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHUOCttJwx51"
      },
      "source": [
        "The Algorithm we use is derived from MIT CSAIL's paper, [\"Eulerian Video Magnification for Revealing Subtle Changes in the World\"](http://people.csail.mit.edu/mrub/papers/vidmag.pdf). I have implemented their paper using Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuKgf2krg0QE"
      },
      "source": [
        "##Overview of the Eulerian video magnification framework\n",
        "\n",
        "\n",
        "![](https://github.com/joeljose/assets/blob/master/EVM/EVM_flow.png?raw=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn4TRM8DxO1j"
      },
      "source": [
        "##There are 5 steps in the algorithm pipeline:\n",
        "1) Loading the video</br>\n",
        "2) Spatial decomposition into laplacian pyramids</br>\n",
        "3) Temporal filtering to extract motion information, and adding that back to the original signal</br>\n",
        "4) Reconstruction </br>\n",
        "5) Saving to output video</br>\n",
        "We will first create helper functions for each of these steps. And finally put them altogether to get our motion magnification method.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade mediapipe opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUQSwsXAeGhj",
        "outputId": "dd1b5ad2-944b-4472-e3e8-ecc22c11e062"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.21 sounddevice-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDJ0VrGZzOEf"
      },
      "source": [
        "## Importing all the essential modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbCbaNBLzW-2"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import scipy.fftpack\n",
        "import scipy.signal\n",
        "from matplotlib import pyplot\n",
        "import requests\n",
        "import mediapipe as mp"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmnIbQnrzeHi"
      },
      "source": [
        "## helper functions for loading and saving videos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DARxncuIzuZw"
      },
      "source": [
        "\n",
        "def uint8_to_float(data):\n",
        "  \"\"\"\n",
        "  converts numpy array of type \"uint8\"(default type when we load through opencv)\n",
        "  to \"float\" datatype.\n",
        "  \"\"\"\n",
        "  result = np.ndarray(shape=data.shape, dtype='float')\n",
        "  result[:] = data * (1. / 255)\n",
        "  return result\n",
        "\n",
        "\n",
        "def float_to_uint8(data):\n",
        "  \"\"\"\n",
        "  converts numpy array of type \"float\" to \"uint8\" datatype.\n",
        "  \"\"\"\n",
        "  result = np.ndarray(shape=data.shape, dtype='uint8')\n",
        "  result[:] = data * 255\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def load_video(video_filename):\n",
        "    \"\"\"Load a video into a numpy array\"\"\"\n",
        "    video_filename = str(video_filename)\n",
        "    print(\"Loading \" + video_filename)\n",
        "    if not os.path.isfile(video_filename):\n",
        "        raise Exception(\"File Not Found: %s\" % video_filename)\n",
        "    capture = cv2.VideoCapture(video_filename)\n",
        "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    width, height = get_capture_dimensions(capture)\n",
        "    fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
        "    x = 0\n",
        "    vid_frames = np.zeros((frame_count, height, width, 3), dtype='uint8')\n",
        "    while capture.isOpened():\n",
        "        ret, frame = capture.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        vid_frames[x] = frame\n",
        "        x += 1\n",
        "    capture.release()\n",
        "\n",
        "    return vid_frames, fps\n",
        "\n",
        "\n",
        "def load_video_float(video_filename):\n",
        "    \"\"\" Returns video as a numpy array of data type float\"\"\"\n",
        "    vid_data, fps = load_video(video_filename)\n",
        "    return uint8_to_float(vid_data), fps\n",
        "\n",
        "\n",
        "def get_capture_dimensions(capture):\n",
        "    \"\"\"Get the dimensions of a capture\"\"\"\n",
        "    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    return width, height\n",
        "\n",
        "def save_video(video, fps, save_filename='output.avi'):\n",
        "    \"\"\"Save a video to disk\"\"\"\n",
        "    video = float_to_uint8(video)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "    writer = cv2.VideoWriter(save_filename, fourcc, fps, (video.shape[2], video.shape[1]), 1)\n",
        "    for x in range(0, video.shape[0]):\n",
        "        res = cv2.convertScaleAbs(video[x])\n",
        "        writer.write(res)\n",
        "    print('output saved to '+save_filename)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROI Extraction"
      ],
      "metadata": {
        "id": "sdfyZkXSkvZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_roi(frame):\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.6)\n",
        "\n",
        "    # Convert BGR to RGB\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    result = pose.process(frame_rgb)\n",
        "\n",
        "    if not result.pose_landmarks:\n",
        "        return None  # No landmarks detected\n",
        "\n",
        "    landmarks = result.pose_landmarks.landmark\n",
        "    h, w = frame.shape[:2]\n",
        "\n",
        "    # Select landmarks relevant for chest-abdomen region\n",
        "    points_of_interest = [\n",
        "        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER],\n",
        "        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER],\n",
        "        landmarks[mp_pose.PoseLandmark.LEFT_HIP],\n",
        "        landmarks[mp_pose.PoseLandmark.RIGHT_HIP]\n",
        "    ]\n",
        "\n",
        "    # Get bounding box coordinates\n",
        "    xs = [int(p.x * w) for p in points_of_interest]\n",
        "    ys = [int(p.y * h) for p in points_of_interest]\n",
        "\n",
        "    x_min, x_max = min(xs), max(xs)\n",
        "    y_min, y_max = min(ys), max(ys)\n",
        "\n",
        "    # Expand slightly the ROI for better coverage\n",
        "    padding_x = int((x_max - x_min) * 0.1)\n",
        "    padding_y = int((y_max - y_min) * 0.1)\n",
        "\n",
        "    x_min = max(0, x_min - padding_x)\n",
        "    x_max = min(w, x_max + padding_x)\n",
        "    y_min = max(0, y_min - padding_y)\n",
        "    y_max = min(h, y_max + padding_y)\n",
        "\n",
        "    roi = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    return roi, (x_min, y_min, x_max, y_max)\n"
      ],
      "metadata": {
        "id": "OJxVPI6pkiG0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNnn1oOP2tMG"
      },
      "source": [
        "##Helper functions for Spatial Decomposition(Laplacian pyramids), Temporal filtering, and Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLEOfvR03VBv"
      },
      "source": [
        "def create_gaussian_image_pyramid(image, pyramid_levels):\n",
        "    \"\"\" Creates a gaussian image pyramid for the input image\"\"\"\n",
        "    gauss_copy = np.ndarray(shape=image.shape, dtype=\"float\")\n",
        "    gauss_copy[:] = image\n",
        "    img_pyramid = [gauss_copy]\n",
        "    for pyramid_level in range(1, pyramid_levels):\n",
        "        gauss_copy = cv2.pyrDown(gauss_copy)\n",
        "        img_pyramid.append(gauss_copy)\n",
        "\n",
        "    return img_pyramid\n",
        "\n",
        "\n",
        "def create_laplacian_image_pyramid(image, pyramid_levels):\n",
        "    \"\"\" Creates a laplacian image pyramid for the input image\"\"\"\n",
        "    gauss_pyramid = create_gaussian_image_pyramid(image, pyramid_levels)\n",
        "    laplacian_pyramid = []\n",
        "    for i in range(pyramid_levels - 1):\n",
        "        laplacian_pyramid.append((gauss_pyramid[i] - cv2.pyrUp(gauss_pyramid[i + 1])))\n",
        "\n",
        "    laplacian_pyramid.append(gauss_pyramid[-1])\n",
        "    return laplacian_pyramid\n",
        "\n",
        "\n",
        "def create_laplacian_video_pyramid(video, pyramid_levels):\n",
        "    \"\"\"Creates a laplacian video pyramid for the input video\"\"\"\n",
        "    vid_pyramid = []\n",
        "    for frame_number, frame in enumerate(video):\n",
        "        frame_pyramid = create_laplacian_image_pyramid(frame, pyramid_levels)\n",
        "\n",
        "        for pyramid_level, pyramid_sub_frame in enumerate(frame_pyramid):\n",
        "            if frame_number == 0:\n",
        "                vid_pyramid.append(\n",
        "                    np.zeros((video.shape[0], pyramid_sub_frame.shape[0], pyramid_sub_frame.shape[1], 3),\n",
        "                                dtype=\"float\"))\n",
        "\n",
        "            vid_pyramid[pyramid_level][frame_number] = pyramid_sub_frame\n",
        "\n",
        "    return vid_pyramid\n",
        "\n",
        "\n",
        "def temporal_bandpass_filter(data, fps, freq_min=0.833, freq_max=1, axis=0, amplification_factor=1):\n",
        "    \"\"\"\n",
        "     filters out the motion information and returns\n",
        "     an amplified version of the motion information.\n",
        "    \"\"\"\n",
        "    fft = scipy.fftpack.rfft(data, axis=axis)\n",
        "    frequencies = scipy.fftpack.fftfreq(data.shape[0], d=1.0 / fps)\n",
        "    bound_low = (np.abs(frequencies - freq_min)).argmin()\n",
        "    bound_high = (np.abs(frequencies - freq_max)).argmin()\n",
        "    fft[:bound_low] = 0\n",
        "    fft[bound_high:-bound_high] = 0\n",
        "    fft[-bound_low:] = 0\n",
        "\n",
        "    result = np.ndarray(shape=data.shape, dtype='float')\n",
        "    result[:] = np.abs(scipy.fftpack.ifft(fft, axis=0))\n",
        "    result *= amplification_factor\n",
        "    return result\n",
        "\n",
        "\n",
        "def collapse_laplacian_pyramid(image_pyramid):\n",
        "    \"\"\"returns the reconstructed image from the input laplacian pyramid\"\"\"\n",
        "    img = image_pyramid.pop()\n",
        "    while image_pyramid:\n",
        "        img = cv2.pyrUp(img) + (image_pyramid.pop() - 0)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def collapse_laplacian_video_pyramid(pyramid):\n",
        "    \"\"\"returns the reconstructed video from the input laplacian video pyramid\"\"\"\n",
        "    i = 0\n",
        "    while True:\n",
        "        try:\n",
        "            img_pyramid = [vid[i] for vid in pyramid]\n",
        "            pyramid[0][i] = collapse_laplacian_pyramid(img_pyramid)\n",
        "            i += 1\n",
        "        except IndexError:\n",
        "            break\n",
        "    return pyramid[0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_KR7Hx97eFS"
      },
      "source": [
        "## Motion magnification function:\n",
        "Inputs to this method are video data(in numpy array format), fps of the video, lower cutoff frequency, upper cutoff frequency, amplification factor, number of levels in the pyramid, number of levels in the top of the pyramid that we skip for filtering.</br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4IzpX7c7pHq"
      },
      "source": [
        "def eulerian_magnification(vid_data, fps, freq_min, freq_max, amplification, pyramid_levels=4, skip_levels_at_top=1):\n",
        "    \"\"\" Returns the motion magnified video\"\"\"\n",
        "    print(\"creating video pyramid....\")\n",
        "    vid_pyramid = create_laplacian_video_pyramid(vid_data, pyramid_levels=pyramid_levels)\n",
        "    for i, vid in enumerate(vid_pyramid):\n",
        "        if i < skip_levels_at_top or i >= len(vid_pyramid) - 1:\n",
        "            # ignore the top and bottom of the pyramid. One end has too much noise and the other end is the\n",
        "            # gaussian representation\n",
        "            continue\n",
        "\n",
        "        bandpassed = temporal_bandpass_filter(vid, fps, freq_min=freq_min, freq_max=freq_max, amplification_factor=amplification)\n",
        "\n",
        "        vid_pyramid[i] += bandpassed\n",
        "\n",
        "    print(\"bandpass filtering \"+str([freq_min,freq_max])+\"Hz...\")\n",
        "    vid_data = collapse_laplacian_video_pyramid(vid_pyramid)\n",
        "    print(\"reconstructing from pyramid...\")\n",
        "    print(\"Done\")\n",
        "    return vid_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQTZVJEu9jJW"
      },
      "source": [
        "## Show Frequencies in the video\n",
        "We have also created a helper function that calculates the mean intensity variation in the input frames of the video. This helps us to figure out the cutoff frequencies required for our video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYANZmwa-SQQ"
      },
      "source": [
        "def show_frequencies(vid_data, fps, bounds=None):\n",
        "    \"\"\"Graph the average value of the video as well as the frequency strength\"\"\"\n",
        "    averages = []\n",
        "\n",
        "    if bounds:\n",
        "        for x in range(1, vid_data.shape[0] - 1):\n",
        "            averages.append(vid_data[x, bounds[2]:bounds[3], bounds[0]:bounds[1], :].sum())\n",
        "    else:\n",
        "        for x in range(1, vid_data.shape[0] - 1):\n",
        "            averages.append(vid_data[x, :, :, :].sum())\n",
        "\n",
        "    averages = averages - min(averages)\n",
        "\n",
        "    charts_x = 1\n",
        "    charts_y = 2\n",
        "    pyplot.figure(figsize=(20, 10))\n",
        "    pyplot.subplots_adjust(hspace=.7)\n",
        "\n",
        "    pyplot.subplot(charts_y, charts_x, 1)\n",
        "    pyplot.title(\"Pixel Average\")\n",
        "    pyplot.xlabel(\"Time\")\n",
        "    pyplot.ylabel(\"Brightness\")\n",
        "    pyplot.plot(averages)\n",
        "\n",
        "    freqs = scipy.fftpack.fftfreq(len(averages), d=1.0 / fps)\n",
        "    fft = abs(scipy.fftpack.fft(averages))\n",
        "    idx = np.argsort(freqs)\n",
        "\n",
        "    pyplot.subplot(charts_y, charts_x, 2)\n",
        "    pyplot.title(\"FFT\")\n",
        "    pyplot.xlabel(\"Freq (Hz)\")\n",
        "    freqs = freqs[idx]\n",
        "    fft = fft[idx]\n",
        "\n",
        "    freqs = freqs[len(freqs) // 2 + 1:]\n",
        "    fft = fft[len(fft) // 2 + 1:]\n",
        "    pyplot.plot(freqs, abs(fft))\n",
        "\n",
        "    pyplot.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn2ZF5bV-YwN"
      },
      "source": [
        "## Downloading the input video to our notebook\n",
        "To do a demo of our motion magnification algorithm we use a video which was used in the original paper. </br>\n",
        "You can alternatively use you own video as input too. You will have to upload the video to the colab notebook, and rename the 'filename' variable as the name of your video.</br>\n",
        "If you want to try your own video, comment the first line in the next code block and Uncomment the next line and enter your own filename.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxRdrFTKEZBX"
      },
      "source": [
        "#filename = \"video.mp4\" # If you want to try your own video, comment this line\n",
        "                      # and Uncomment the next line and enter your own filename\n",
        "\n",
        "filename=\"Original.mp4\"\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcwlbXbiCQqH"
      },
      "source": [
        "##helper function for downloading video\n",
        "You can skip the downloading part in this notebook if you are using your own video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W20NKOBICbIS"
      },
      "source": [
        "def download_file(url, dest_filename):\n",
        "    \"\"\"Downloads the file in given url\"\"\"\n",
        "    if os.path.isfile(dest_filename):\n",
        "        print('Already Downloaded: %s to %s' % (url, dest_filename))\n",
        "        return\n",
        "    print('Downloading: %s to %s' % (url, dest_filename))\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    if not response.ok:\n",
        "        raise Exception(\"Couldn't download file\")\n",
        "\n",
        "    with open(dest_filename, 'wb') as fp:\n",
        "        for block in response.iter_content(1024):\n",
        "            fp.write(block)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNdignSEDsF7"
      },
      "source": [
        "## Downloading....\n",
        "Skip the next code block if you are using your own video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiQWMlz2D875",
        "outputId": "69bd7a45-19d1-4d54-e45a-b192b9931103"
      },
      "source": [
        "download_file('http://people.csail.mit.edu/mrub/evm/video/face.mp4',filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: http://people.csail.mit.edu/mrub/evm/video/face.mp4 to video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtPYvWcnF52_"
      },
      "source": [
        "## Final Demo on \"input.mp4\"\n",
        "load the video and use show_frequencies to see the frequency range of the input video."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video_with_roi(video_filename, roi=(50, 50, 200, 200)):  # Example ROI (x, y, w, h)\n",
        "    cap = cv2.VideoCapture(video_filename)\n",
        "\n",
        "    frames_roi = []\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # End of video\n",
        "\n",
        "        # Extract ROI (Make sure the coordinates are correct)\n",
        "        x, y, w, h = roi\n",
        "        roi_frame = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize ROI to ensure consistency\n",
        "        roi_resized = cv2.resize(roi_frame, (w, h))  # Resize to fixed (width, height)\n",
        "\n",
        "        frames_roi.append(roi_resized)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    vid_data = np.array(frames_roi, dtype=np.uint8)  # Explicit dtype\n",
        "\n",
        "    return vid_data, fps"
      ],
      "metadata": {
        "id": "9ndCcG-nlGIq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "Z37Zf1a6Hoxn",
        "outputId": "9f4be82f-832d-494a-993c-9ebdf7584201"
      },
      "source": [
        "# Now, let's load video with ROI extraction\n",
        "vid, fps = load_video_with_roi(filename)\n",
        "show_frequencies(vid, fps)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (576,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7029b1c7fb31>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now, let's load video with ROI extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_video_with_roi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshow_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-09f6156d268e>\u001b[0m in \u001b[0;36mload_video_with_roi\u001b[0;34m(video_filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mvid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_roi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (576,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8Ryk9evH7KH"
      },
      "source": [
        "You can use this information for making changes in the cutoff frequencies in the next code block if you want to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlUGWzfvG5ho"
      },
      "source": [
        "lower_hertz = 0.5\n",
        "upper_hertz = 2\n",
        "amplification_factor = 50\n",
        "pyramid_levels = 4"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd5XZNaEJgmv"
      },
      "source": [
        "Motion magnify the loaded video:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRHuQ5JdGSLo",
        "outputId": "55e4080f-0cf3-4543-9edd-64628d03cdd1"
      },
      "source": [
        "mag_vid = eulerian_magnification(\n",
        "    vid, fps,\n",
        "    freq_min=lower_hertz,\n",
        "    freq_max=upper_hertz,\n",
        "    amplification=amplification_factor,\n",
        "    pyramid_levels=pyramid_levels\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating video pyramid....\n",
            "bandpass filtering [0.5, 2]Hz...\n",
            "reconstructing from pyramid...\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh6gg9zoJpCx"
      },
      "source": [
        "save video to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq-sOm79JpqG",
        "outputId": "5f25ba66-0305-460b-d0a6-cdea254f1d6a"
      },
      "source": [
        "out_name = filename.split(\".\")[0] + \"_levels_\" + str(pyramid_levels) + \"_min_\" + str(\n",
        "    lower_hertz) + \"_max_\" + str(upper_hertz) + \"_amp_\" + str(amplification_factor)+'.avi'\n",
        "\n",
        "save_video(mag_vid, fps, out_name)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output saved to Original_levels_4_min_0.5_max_2_amp_50.avi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OtHqysPKBfh"
      },
      "source": [
        "##Conclusion\n",
        "To amplify motion, EVM does not perform feature\n",
        "tracking or optical flow computation, but merely magnifies temporal color changes using spatio-temporal processing. This Eulerian based method, which temporally processes pixels in a fixed spatial\n",
        "region, successfully reveals informative signals and amplifies small motions in real-world videos.\n",
        "\n",
        "One drawback of this method is that we can see that we get artifacts in our videos as we increase amplification factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtPSb6paSisi"
      },
      "source": [
        "\n",
        "## If you like this content, follow me\n",
        "<a href=\"https://twitter.com/joelk1jose\" target=\"_blank\"><img class=\"ai-subscribed-social-icon\" src=\"https://github.com/joeljose/assets/blob/master/images/tw.png?raw=True\" width=\"30\"></a>\n",
        "<a href=\"https://github.com/joeljose\" target=\"_blank\"><img class=\"ai-subscribed-social-icon\" src=\"https://github.com/joeljose/assets/blob/master/images/gthb.png?raw=True\" width=\"30\"></a>\n",
        "<a href=\"https://www.linkedin.com/in/joel-jose-527b80102/\" target=\"_blank\"><img class=\"ai-subscribed-social-icon\" src=\"https://github.com/joeljose/assets/blob/master/images/lnkdn.png?raw=True\" width=\"30\"></a>\n",
        "\n",
        "<h3 align=\"center\">Show your support by starring the repository 🙂</h3>"
      ]
    }
  ]
}